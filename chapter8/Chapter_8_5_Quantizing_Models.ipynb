{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Chapter 7.5 Quantizing Models.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "QcxNUJb99ONb",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "outputId": "0079f6b4-a609-4efc-8a09-a392a709ac70",
    "ExecuteTime": {
     "end_time": "2023-11-11T08:50:03.413964Z",
     "start_time": "2023-11-11T08:50:00.453630Z"
    }
   },
   "source": [
    "!pip install torch transformers"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: transformers in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (4.31.0)\r\n",
      "Requirement already satisfied: filelock in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (0.17.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (1.26.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (0.13.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (0.4.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjIYt4QS9YWL",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0kxCNnI9RVc",
    "colab_type": "text"
   },
   "source": [
    "# Big Models Hate This One Weird Trick! (Quantization, T5, & PyTorch 1.4)\n",
    "\n",
    "As we know, models can be big lumbering beasts, comprised of millions of parameters (both weights and activations) that require lots of matrix multiplications to take an input and arrive at an answer. And for most of our work so far, that's been fine! We have mighty GPUs that can handle these burdens with ease.\n",
    "\n",
    "But what if we didn’t? We often package a model up for production inference usage so that it only runs on the CPU. And what if we wanted to run our model on a smaller embedded platform? Suddenly, both the size of the model and all those floating-point operations become a little more problematic. Thankfully, there’s a trick we can perform that makes our model smaller _and_ faster, normally with the trade off with some accuracy. Even better, PyTorch allows us to perform this one weird trick with just one line of code, with some other approaches for squeezing even more performance. Let’s have a quick look at _quantization_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyNq8LN_9idf",
    "colab_type": "text"
   },
   "source": [
    "## Quantization\n",
    "\n",
    "Every parameter in our model is a 32-bit floating point number, taking up 4 bytes of memory. That’s not a lot, but it can soon add up. Let's have a look at Google’s recent T5 transformer-based model, which has a `t5-small` variant that’s available in the `transformers` library. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yDxF0i309uJy",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "f47acad5-c8d7-4fc7-d0cf-a2cee9a536a2",
    "ExecuteTime": {
     "end_time": "2023-11-11T09:02:53.139303Z",
     "start_time": "2023-11-11T09:02:51.183951Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import pipeline, T5ForConditionalGeneration\n",
    "\t\t\n",
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters())\n",
    "\t    \n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(\"cpu\")\n",
    "\n",
    "param_count = count_parameters(base_model)\n",
    "\n",
    "memory = (param_count * 4) / (1024 *1024)\n",
    "memory\n"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "230.814453125"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzzcwPQW-NW_",
    "colab_type": "text"
   },
   "source": [
    "Even with the smallest pre-trained T5 weights, our model is roughly 60m parameters and weighs in at a whopping 230Mb! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY079U2m-Wh2",
    "colab_type": "text"
   },
   "source": [
    "However, what if we decided that we didn’t need the full precision of our floating-point parameters? If our parameters could be restricted to within a certain range of values, then we could use a smaller type of number representation to store the parameters. This _quantization_ is the key to speeding up our inference time and reducing the memory footprint of our models. What we tend to aim for is to quantize down from a 32-bit floating point to an 8-bit integer. The basic idea is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRM0VuTr-Y0k",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "$x_{int8} = (\\frac{x_{float32}}{x_{scale}} + x_{offset})$\n",
    "\n",
    "Which is essentially just fitting the potential values of the parameters of a network to a line of $y = mx + c$, although due to the reduced resolution of the 8-bit integer, there's only so many values a parameter now may take instead of the huge amount that a `float32` value could be. PyTorch does its quantizing in a slightly more complicated affair that ensures that zero is always zero, but the basic idea is the same - we have a range of values that our parameters can take, and then find an appropriate pair $x_{scale}$ and $x_{offset}$ to provide 256 graduations to represent that range - or 255 if you think about PyTorch always keeping zero around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUJNQivcAVVZ",
    "colab_type": "text"
   },
   "source": [
    "At the moment (PyTorch 1.5), quantized layers are best supported with `CNN` and `Linear` layers. Thankfully, if we have a look at the model structure of T5, we can see a happy coincidence:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "heo9StPOAZEL",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "4855ce94-7df0-473f-f1ec-b352925f3580",
    "ExecuteTime": {
     "end_time": "2023-11-11T09:02:57.120257Z",
     "start_time": "2023-11-11T09:02:57.112226Z"
    }
   },
   "source": [
    "base_model"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGSvW-kkAfyS",
    "colab_type": "text"
   },
   "source": [
    "Yes, that’s right, look at all those `Linear` layers! We should be able to get some benefit out of quantizing this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYznffbOAuSl",
    "colab_type": "text"
   },
   "source": [
    "## One Weird Trick — Dynamic Quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qnnpack', 'none']\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.quantized\n",
    "print(torch.backends.quantized.supported_engines)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:03:02.957254Z",
     "start_time": "2023-11-11T09:03:02.952637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U8YLMB4KAvKL",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2023-11-11T09:06:00.396256Z",
     "start_time": "2023-11-11T09:06:00.176534Z"
    }
   },
   "source": [
    "import torch.quantization\n",
    "import torch\n",
    "\n",
    "# Assuming base_model is on GPU, move it to CPU\n",
    "base_model.to('cpu')\n",
    "\n",
    "# Perform dynamic quantization on CPU\n",
    "quantized_model = torch.quantization.quantize_dynamic(base_model, {torch.nn.Linear}, dtype=torch.qint8)\n"
   ],
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0DEsvrmA5kp",
    "colab_type": "text"
   },
   "source": [
    "No, really, that’s it. Chapter done. Bye!\n",
    "\n",
    "Oh, okay, if you really insist. But honestly, there’s not much more to it. Okay, firstly, a caveat in that `quantize_dynamic` will only quantize the weights, not the activations in our parameters. But all we need to do is pass in the `model` we wish to quantize and a dict of layers that we wish to replace with our quantized versions, in this case `Linear`. The function returns a new model, though you could run with the optional parameter `inplace=True` to mutate the original model rather than make a copy. \n",
    "\n",
    "Let’s save the model and take a look at the quantized size:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-8t9Hq62A9Sm",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "e2abd9b4-2dce-437a-e530-f53dc6e83938",
    "ExecuteTime": {
     "end_time": "2023-11-11T09:07:39.056608Z",
     "start_time": "2023-11-11T09:07:38.839401Z"
    }
   },
   "source": [
    "!mkdir t5\n",
    "\n",
    "quantized_model.to('cpu').save_pretrained(\"t5\")\n",
    "!du -m t5"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: t5: File exists\r\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmkdir t5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m quantized_model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt5\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdu -m t5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/modeling_utils.py:1818\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, **kwargs)\u001B[0m\n\u001B[1;32m   1815\u001B[0m weights_name \u001B[38;5;241m=\u001B[39m SAFE_WEIGHTS_NAME \u001B[38;5;28;01mif\u001B[39;00m safe_serialization \u001B[38;5;28;01melse\u001B[39;00m WEIGHTS_NAME\n\u001B[1;32m   1816\u001B[0m weights_name \u001B[38;5;241m=\u001B[39m _add_variant(weights_name, variant)\n\u001B[0;32m-> 1818\u001B[0m shards, index \u001B[38;5;241m=\u001B[39m shard_checkpoint(state_dict, max_shard_size\u001B[38;5;241m=\u001B[39mmax_shard_size, weights_name\u001B[38;5;241m=\u001B[39mweights_name)\n\u001B[1;32m   1820\u001B[0m \u001B[38;5;66;03m# Clean the folder from a previous save\u001B[39;00m\n\u001B[1;32m   1821\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(save_directory):\n",
      "File \u001B[0;32m~/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/modeling_utils.py:314\u001B[0m, in \u001B[0;36mshard_checkpoint\u001B[0;34m(state_dict, max_shard_size, weights_name)\u001B[0m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 314\u001B[0m     storage_id \u001B[38;5;241m=\u001B[39m id_tensor_storage(weight)\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# If a `weight` shares the same underlying storage as another tensor, we put `weight` in the same `block`\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m storage_id \u001B[38;5;129;01min\u001B[39;00m storage_id_to_block:\n",
      "File \u001B[0;32m~/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/pytorch_utils.py:287\u001B[0m, in \u001B[0;36mid_tensor_storage\u001B[0;34m(tensor)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mid_tensor_storage\u001B[39m(tensor: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mdevice, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m    281\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;124;03m    Unique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;124;03m    example, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\u001B[39;00m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;124;03m    guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\u001B[39;00m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;124;03m    non-overlapping lifetimes may have the same id.\u001B[39;00m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 287\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mdevice, storage_ptr(tensor), storage_size(tensor)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'torch.dtype' object has no attribute 'device'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4AwvS1CBXzN",
    "colab_type": "text"
   },
   "source": [
    "Almost a 50% reduction in size! We can’t get down to 4 times smaller due to not being able to store the activations as 8-bit integers, but we’ve done pretty well for one line of code. Let's do a very simple microbenchmark using both models in the `transformers` library summarization pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-EydnHY3BbwT",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2023-11-11T09:02:07.616596Z",
     "start_time": "2023-11-11T09:02:02.735079Z"
    }
   },
   "source": [
    "base_summarizer = pipeline(\"summarization\", model=base_model, tokenizer=\"t5-small\")\n",
    "quantized_summarizer = pipeline(\"summarization\", model=quantized_model, tokenizer=\"t5-small\")"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29b46f3c912d4cd9964955ecc0d76085"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75bc7c2b7fe64458a4455f024d41da4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74ca0de36ae24441b7c91380cf4ed5da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1099, in _get_module\n",
      "    # docstyle-ignore\n",
      "                      \n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py\", line 39, in <module>\n",
      "    from ...utils import (\n",
      "ImportError: cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/74/rcykwfcx5bq6vwfd93bqrhgm0000gn/T/ipykernel_26597/2471930987.py\", line 1, in <module>\n",
      "    base_summarizer = pipeline(\"summarization\", model=base_model, tokenizer=\"t5-small\")\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/pipelines/__init__.py\", line 988, in pipeline\n",
      "    )\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py\", line 67, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 955, in check_model_type\n",
      "    elif isinstance(inputs, torch.Tensor):\n",
      "                             ^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 708, in items\n",
      "    raise ValueError(f\"Could not find {attr} neither in {module} nor in {transformers_module}!\")\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 711, in <listcomp>\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 680, in _load_attr_from_module\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 625, in getattribute_from_module\n",
      "    )\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1089, in __getattr__\n",
      "    \"\"\"\n",
      "        \n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1101, in _get_module\n",
      "    {0} requires the PIL library but it was not found in your environment. You can install it with pip:\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Failed to import transformers.models.bart.modeling_bart because of the following error (look up to see its traceback):\n",
      "cannot import name 'is_flash_attn_2_available' from 'transformers.utils' (/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BHhiTSOlBg-f",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "ee37076a-777e-4af2-ef18-da7048ece96b",
    "ExecuteTime": {
     "end_time": "2023-11-11T09:02:11.947755Z",
     "start_time": "2023-11-11T09:02:11.842715Z"
    }
   },
   "source": [
    "%timeit base_summarizer(\"From the very beginning, Regan was seen as having series potential. After the television film scored highly in the ratings, work began on the development of the series proper. Ian Kennedy Martin's idea was for the series to be mainly studio-based, with more dialogue and less action, but producer Ted Childs disagreed, and in consequence Ian Kennedy Martin parted company with the project. Childs produced it on 16mm film, a format that allowed for a much smaller film unit than videotape at that time. This made it possible to shoot almost entirely on location which helped give the series a startling degree of realism and to use film editing techniques which enabled him to give the show a heavy bias toward action sequences. The television play and the subsequent series were commissioned by Thames Television and produced by its film division Euston Films. It was originally broadcast on ITV between 2 January 1975 and 28 December 1978 at 21:00–22:00 on weekdays (usually Mondays), with repeated screenings at the same time until the early 1980s. The writers were given strict guidelines to follow: \\\"Each show will have an overall screen time (minus titles) of 48 minutes 40 seconds. Each film will open with a teaser of up to 3 minutes, which will be followed by the opening titles. The story will be played across three acts, each being no more than 19 minutes and no less than 8 minutes in length. Regan will appear in every episode, Carter in approximately 10 out of 13 episodes. In addition to these main characters, scripts should be based around three major speaking parts, with up to ten minor speaking parts\")"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_summarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimeit\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbase_summarizer(\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFrom the very beginning, Regan was seen as having series potential. After the television film scored highly in the ratings, work began on the development of the series proper. Ian Kennedy Martin\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124ms idea was for the series to be mainly studio-based, with more dialogue and less action, but producer Ted Childs disagreed, and in consequence Ian Kennedy Martin parted company with the project. Childs produced it on 16mm film, a format that allowed for a much smaller film unit than videotape at that time. This made it possible to shoot almost entirely on location which helped give the series a startling degree of realism and to use film editing techniques which enabled him to give the show a heavy bias toward action sequences. The television play and the subsequent series were commissioned by Thames Television and produced by its film division Euston Films. It was originally broadcast on ITV between 2 January 1975 and 28 December 1978 at 21:00–22:00 on weekdays (usually Mondays), with repeated screenings at the same time until the early 1980s. The writers were given strict guidelines to follow: \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEach show will have an overall screen time (minus titles) of 48 minutes 40 seconds. Each film will open with a teaser of up to 3 minutes, which will be followed by the opening titles. The story will be played across three acts, each being no more than 19 minutes and no less than 8 minutes in length. Regan will appear in every episode, Carter in approximately 10 out of 13 episodes. In addition to these main characters, scripts should be based around three major speaking parts, with up to ten minor speaking parts\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2432\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2430\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[1;32m   2431\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2432\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2434\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2435\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2436\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m~/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/magics/execution.py:1185\u001B[0m, in \u001B[0;36mExecutionMagics.timeit\u001B[0;34m(self, line, cell, local_ns)\u001B[0m\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m   1184\u001B[0m     number \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m index\n\u001B[0;32m-> 1185\u001B[0m     time_number \u001B[38;5;241m=\u001B[39m timer\u001B[38;5;241m.\u001B[39mtimeit(number)\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m time_number \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m:\n\u001B[1;32m   1187\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/learn-env/lib/python3.11/site-packages/IPython/core/magics/execution.py:173\u001B[0m, in \u001B[0;36mTimer.timeit\u001B[0;34m(self, number)\u001B[0m\n\u001B[1;32m    171\u001B[0m gc\u001B[38;5;241m.\u001B[39mdisable()\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 173\u001B[0m     timing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minner(it, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimer)\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gcold:\n",
      "File \u001B[0;32m<magic-timeit>:1\u001B[0m, in \u001B[0;36minner\u001B[0;34m(_it, _timer)\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'base_summarizer' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-am4cPghCdtc",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "6e0b1484-ffe2-4523-85da-e4d00884a74e"
   },
   "source": [
    "%timeit quantized_summarizer(\"From the very beginning, Regan was seen as having series potential. After the television film scored highly in the ratings, work began on the development of the series proper. Ian Kennedy Martin's idea was for the series to be mainly studio-based, with more dialogue and less action, but producer Ted Childs disagreed, and in consequence Ian Kennedy Martin parted company with the project. Childs produced it on 16mm film, a format that allowed for a much smaller film unit than videotape at that time. This made it possible to shoot almost entirely on location which helped give the series a startling degree of realism and to use film editing techniques which enabled him to give the show a heavy bias toward action sequences. The television play and the subsequent series were commissioned by Thames Television and produced by its film division Euston Films. It was originally broadcast on ITV between 2 January 1975 and 28 December 1978 at 21:00–22:00 on weekdays (usually Mondays), with repeated screenings at the same time until the early 1980s. The writers were given strict guidelines to follow: \\\"Each show will have an overall screen time (minus titles) of 48 minutes 40 seconds. Each film will open with a teaser of up to 3 minutes, which will be followed by the opening titles. The story will be played across three acts, each being no more than 19 minutes and no less than 8 minutes in length. Regan will appear in every episode, Carter in approximately 10 out of 13 episodes. In addition to these main characters, scripts should be based around three major speaking parts, with up to ten minor speaking parts\")"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 16.6 s per loop\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oANUgOHTEDb0",
    "colab_type": "text"
   },
   "source": [
    "In addition to almost being half the size, the quantized model is almost twice as fast! So…why don’t we do this _all_ the time? Are there no downsides? Well…it depends. We **are** losing information in our inference in a quantized model as our values cannot map to all the possible floating-point values that we find in the original model. So the chain of multiplications will be less accurate in our quantized model than in the original. You’ll need to check the new model against a reference dataset to determine the accuracy loss and whether that loss is an acceptable trade-off compared to the reduced storage demands and faster execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4IgTYAAEiI2",
    "colab_type": "text"
   },
   "source": [
    "## Other Quantizing Options Are Available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StgyLod9EtEl",
    "colab_type": "text"
   },
   "source": [
    "In addition to dynamic quantizing, PyTorch also offers _static quantizing_, where a trained model is modified to include _observer_ modules and a selection of data is fed into the model. During the inference on this data, the observers can generate a quantized distribution that fits best to the observed data and the activations that result. This can can produce even further space and time savings, especially with vision models like ResNet.\n",
    "\n",
    "However, for the best-in-class of accuracy in your smaller model, you'll want to investigate quantization-aware training (QAT). In this approach, the model fakes quantizing during the training loop of both the forward and backward passes; while all the computations take place with standard floats, everything is rounded down to integer values, so you end up with a quantized model after training is finished, but one with a higher accuracy than you can acheive with the dynamic or static approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ufUnEbPHoNs",
    "colab_type": "text"
   },
   "source": [
    "## Is It Worth It?\n",
    "\n",
    "You might be wondering if you're just better off training a smaller model rather than going to all this effort to compress larger models. In the recent paper, [Train Large, Then Compress](https://arxiv.org/abs/2002.11794), there's a good deal of evidence presented that transformer-based models really do benefit from this approach. Because larger models converge faster than smaller ones, you will likely get more accurate results by training a large model and compressing than if you spent the same compute time on a smaller model. So go forth and compress!\n",
    "\n",
    "(and we'll see you back here in the future for _pruning_ models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NbH-K1IHkEC",
    "colab_type": "text"
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "https://arxiv.org/abs/2002.11794"
   ]
  }
 ]
}
