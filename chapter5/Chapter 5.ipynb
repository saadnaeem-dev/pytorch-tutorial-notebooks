{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (0.16.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torchtext) (4.65.0)\r\n",
      "Requirement already satisfied: requests in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torchtext) (2.31.0)\r\n",
      "Requirement already satisfied: torch==2.1.0 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torchtext) (2.1.0)\r\n",
      "Requirement already satisfied: numpy in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torchtext) (1.26.0)\r\n",
      "Requirement already satisfied: torchdata==0.7.0 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torchtext) (0.7.0)\r\n",
      "Requirement already satisfied: filelock in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch==2.1.0->torchtext) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch==2.1.0->torchtext) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch==2.1.0->torchtext) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch==2.1.0->torchtext) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch==2.1.0->torchtext) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torch==2.1.0->torchtext) (2023.6.0)\r\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from torchdata==0.7.0->torchtext) (1.26.16)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from requests->torchtext) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from requests->torchtext) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from requests->torchtext) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from jinja2->torch==2.1.0->torchtext) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/saadnaeem/anaconda3/envs/learn-env/lib/python3.11/site-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext\n",
    "# !pip install torch==1.8.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-30T05:03:44.595209Z",
     "start_time": "2023-10-30T05:03:43.106172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T05:04:03.084010Z",
     "start_time": "2023-10-30T05:03:53.887149Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T05:05:54.846324Z",
     "start_time": "2023-10-30T05:05:54.839675Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"mps\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T05:14:53.118262Z",
     "start_time": "2023-10-31T05:14:53.081106Z"
    }
   },
   "outputs": [],
   "source": [
    "# You'll probably need to use the 'python' engine to load the CSV\n",
    "# tweetsDF = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None)\n",
    "tweetsDF = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "   0           1                             2         3                4  \\\n0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n\n                                                   5  \n0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1  is upset that he can't update his Facebook by ...  \n2  @Kenichan I dived many times for the ball. Man...  \n3    my whole body feels itchy and like its on fire   \n4  @nationwideclass no, it's not behaving at all....  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T05:16:52.826976Z",
     "start_time": "2023-10-31T05:16:52.811423Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T05:19:22.594185Z",
     "start_time": "2023-10-31T05:19:22.586300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0\n4    10001\n0     9999\nName: count, dtype: int64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tweetsDF[\"sentiment_cat\"] = tweetsDF[0].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T05:23:49.505890Z",
     "start_time": "2023-10-31T05:23:49.500545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "0        0\n1        0\n2        0\n3        0\n4        0\n        ..\n19995    4\n19996    4\n19997    4\n19998    4\n19999    4\nName: sentiment_cat, Length: 20000, dtype: category\nCategories (2, int64): [0, 4]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF[\"sentiment_cat\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T05:23:59.501786Z",
     "start_time": "2023-10-31T05:23:59.498021Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "tweetsDF[\"sentiment\"] = tweetsDF[\"sentiment_cat\"].cat.codes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T05:25:22.290843Z",
     "start_time": "2023-10-31T05:25:22.289201Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "0        0\n1        0\n2        0\n3        0\n4        0\n        ..\n19995    1\n19996    1\n19997    1\n19998    1\n19999    1\nName: sentiment, Length: 20000, dtype: int8"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF[\"sentiment\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T05:25:23.100900Z",
     "start_time": "2023-10-31T05:25:23.097894Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T05:27:06.191123Z",
     "start_time": "2023-10-31T05:27:06.099357Z"
    }
   },
   "outputs": [],
   "source": [
    "tweetsDF.to_csv(\"train-processed.csv\", header=None, index=None)      \n",
    "tweetsDF.sample(10000).to_csv(\"train-processed-sample.csv\", header=None, index=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField()\n",
    "TWEET = data.Field('spacy', tokenizer_language='en_core_web_sm', lower=True)\n",
    "\n",
    "fields = [('score',None), ('id',None), ('date',None), ('query',None),\n",
    "          ('name',None), ('tweet', TWEET), ('category',None), ('label',LABEL)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterDataset = data.dataset.TabularDataset(\n",
    "        path=\"train-processed-sample.csv\", \n",
    "        format=\"CSV\", \n",
    "        fields=fields,\n",
    "        skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 2000, 2000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train, test, valid) = twitterDataset.split(split_ratio=[0.6,0.2,0.2],\n",
    "                                            stratified=True, strata_field='label')\n",
    "\n",
    "(len(train),len(test),len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 3742),\n",
       " ('!', 3315),\n",
       " ('.', 3084),\n",
       " (' ', 2175),\n",
       " ('to', 2115),\n",
       " ('the', 2022),\n",
       " (',', 1823),\n",
       " ('a', 1461),\n",
       " ('my', 1205),\n",
       " ('it', 1197)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "TWEET.build_vocab(train, max_size = vocab_size)\n",
    "LABEL.build_vocab(train)\n",
    "TWEET.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size = 32,\n",
    "    device = device,\n",
    "    sort_key = lambda x: len(x.tweet),\n",
    "    sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OurFirstLSTM(\n",
       "  (embedding): Embedding(20002, 300)\n",
       "  (encoder): LSTM(300, 100)\n",
       "  (predictor): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OurFirstLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "        super(OurFirstLSTM, self).__init__()\n",
    "    \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim,  \n",
    "                hidden_size=hidden_size, num_layers=1)\n",
    "        self.predictor = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        output, (hidden,_) = self.encoder(self.embedding(seq))\n",
    "        preds = self.predictor(hidden.squeeze(0))\n",
    "        return preds\n",
    "\n",
    "model = OurFirstLSTM(100,300, 20002)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    for epoch in range(1, epochs+1):\n",
    "     \n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.tweet)\n",
    "            loss = criterion(predict,batch.label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * batch.tweet.size(0)\n",
    "        training_loss /= len(train_iterator)\n",
    " \n",
    "        \n",
    "        model.eval()\n",
    "        for batch_idx,batch in enumerate(valid_iterator):\n",
    "            predict = model(batch.tweet)\n",
    "            loss = criterion(predict,batch.label)\n",
    "            valid_loss += loss.data.item() * batch.tweet.size(0)\n",
    " \n",
    "        valid_loss /= len(valid_iterator)\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}'.format(epoch, training_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 24.47, Validation Loss: 14.04\n",
      "Epoch: 2, Training Loss: 23.81, Validation Loss: 14.57\n",
      "Epoch: 3, Training Loss: 23.25, Validation Loss: 15.69\n",
      "Epoch: 4, Training Loss: 23.12, Validation Loss: 16.16\n",
      "Epoch: 5, Training Loss: 21.71, Validation Loss: 18.80\n"
     ]
    }
   ],
   "source": [
    "train(5, model, optimizer, criterion, train_iterator, valid_iterator)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(tweet):\n",
    "    categories = {0: \"Negative\", 1:\"Positive\"}\n",
    "    processed = TWEET.process([TWEET.preprocess(tweet)])\n",
    "    processed = processed.to(device)\n",
    "    model.eval()\n",
    "    return categories[model(processed).argmax().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(words, p=0.5):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words))\n",
    "    if len(remaining) == 0:\n",
    "        return [random.choice(words)]\n",
    "    else:\n",
    "        return remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(sentence, n=5):\n",
    "    length = range(len(sentence))\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(length, 2)\n",
    "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you'll have to define remove_stopwords() and get_synonyms() elsewhere\n",
    "\n",
    "def random_insertion(sentence,n):\n",
    "    words = remove_stopwords(sentence)\n",
    "    for _ in range(n):\n",
    "        new_synonym = get_synonyms(random.choice(words))\n",
    "        sentence.insert(randrange(len(sentence)+1), new_synonym)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install googletrans version 3.1.0a0 (temporary fix for #57)\n",
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import googletrans\n",
    "import random\n",
    "\n",
    "translator = googletrans.Translator()\n",
    "\n",
    "sentences = ['The cat sat on the mat']\n",
    "\n",
    "translations_fr = translator.translate(sentences, dest='fr')\n",
    "fr_text = [t.text for t in translations_fr] \n",
    "translations_en = translator.translate(fr_text, dest='en')\n",
    "en_text = [t.text for t in translations_en]\n",
    "print(en_text)   \n",
    "\n",
    "available_langs = list(googletrans.LANGUAGES.keys())\n",
    "tr_lang = random.choice(available_langs)\n",
    "print(f\"Translating to {googletrans.LANGUAGES[tr_lang]}\")\n",
    "\n",
    "translations = translator.translate(sentences, dest=tr_lang)\n",
    "t_text = [t.text for t in translations]\n",
    "print(t_text)\n",
    "\n",
    "translations_en_random = translator.translate(t_text, src=tr_lang, dest='en')\n",
    "en_text = [t.text for t in translations_en_random]\n",
    "print(en_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
